# -*- coding: utf-8 -*-
"""coding_solution_Somrupa_Sarkar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A4xf4EFz5r5_x4G9lqIQrXZmNSUbo6Fx
"""

pip install datasets

# Importing necessary libraries
from datasets import load_dataset

# Load the IMDb dataset from Hugging Face
imdb_dataset = load_dataset('imdb')
# Inspect the dataset
print(imdb_dataset)

from transformers import AutoTokenizer
# Load the pre-trained Hugging Face tokenizer
hf_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

import spacy

# Load SpaCy's small English model
nlp = spacy.load("en_core_web_sm")

# Tokenizing with Hugging Face tokenizer with truncation
hf_tokens_list = []
for text in imdb_dataset['train']['text'][:1000]:  # Tokenize the first 1000 examples
    hf_tokens = hf_tokenizer.tokenize(text, truncation=True, max_length=512)
    hf_tokens_list.extend(hf_tokens)

# Tokenizing with SpaCy tokenizer
spacy_tokens_list = []
for doc in nlp.pipe(imdb_dataset['train']['text'][:1000]):  # Tokenize the first 1000 examples
    spacy_tokens = [token.text for token in doc]
    spacy_tokens_list.extend(spacy_tokens)

from collections import Counter
import math

# Function to calculate entropy
def calculate_entropy(tokens):
    token_freqency = Counter(tokens)
    total_tokens = len(tokens)
    entropy = 0
    for freq in token_freqency.values():
        prob = freq / total_tokens
        entropy -= prob * math.log2(prob)
    return entropy
    hf_entropy = calculate_entropy(hf_tokens_list)
spacy_entropy = calculate_entropy(spacy_tokens_list)

print(f"Hugging Face tokenizer entropy: {hf_tokens_list}")
print(f"SpaCy tokenizer entropy: {spacy_tokens_list}")